# ğŸ§  RAG Document QA System (FastAPI + OpenAI + FAISS + Ollama)

A **Retrieval-Augmented Generation (RAG)** API built with **FastAPI**, combining:

- **OpenAI embeddings** (for retrieval)
- **FAISS** (for fast vector search)
- **Ollama** (for local LLM answer generation)
- **Recursive chunking** via LangChain
- **Modular service architecture**
```

## ğŸ—ï¸ Project Structure
app/
â”œâ”€ main.py
â”œâ”€ api/
â”‚ â”œâ”€ upload.py # Upload & process documents
â”‚ â”œâ”€ query.py # Query documents
â”‚ â””â”€ documents.py # List / delete indexed docs
â”œâ”€ services/
â”‚ â”œâ”€ file_readers.py # PDF/TXT extraction
â”‚ â”œâ”€ docs_service.py # Chunking + embedding
â”‚ â”œâ”€ embedding_service.py
â”‚ â”œâ”€ vector_service.py # FAISS vector storage
â”‚ â””â”€ llm_service.py # Ollama LLM integration
â”œâ”€ core/
â”‚ â”œâ”€ config.py # Env vars & constants
â”‚ â””â”€ logging_config.py # Logging setup
â””â”€ experiments/
â””â”€ compare_chunk_sizes.py


---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the repository
```bash
git clone git@github.com:Titanflame26/Assignment-3.git


2ï¸âƒ£ Create a virtual environment

python -m venv venv
source venv/bin/activate      # (Linux/Mac)
venv\Scripts\activate         # (Windows)

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt
4ï¸âƒ£ Create .env file
# --- OpenAI ---
OPENAI_API_KEY=sk-xxxxxxx
EMBEDDING_MODEL=text-embedding-3-small

# --- Ollama (local LLM) ---
OLLAMA_MODEL=llama3
OLLAMA_BASE_URL=http://localhost:11434

# --- FAISS / Storage ---
DATA_DIR=./data/index
TOP_K=4
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# --- App Settings ---
ENVIRONMENT=development
LOG_LEVEL=INFO

ğŸ§© Running Ollama
ollama serve
ollama pull llama3

ğŸš€ Run the FastAPI App

Start the API server:

uvicorn app.main:app --reload

Swagger UI â†’ http://127.0.0.1:8000/docs

Health Check â†’ http://127.0.0.1:8000/
